{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e60c15b",
   "metadata": {},
   "source": [
    "# Create Wikipedia Topic Mix Dataset for CVDD\n",
    "This notebook builds a new anomaly detection dataset by scraping Wikipedia articles for normal and anomalous topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84ed4134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wikipedia-api in /opt/anaconda3/envs/cvdd/lib/python3.10/site-packages (0.8.1)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/cvdd/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/cvdd/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/cvdd/lib/python3.10/site-packages (4.67.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/cvdd/lib/python3.10/site-packages (1.5.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/cvdd/lib/python3.10/site-packages (from wikipedia-api) (2.32.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/cvdd/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/cvdd/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/cvdd/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/anaconda3/envs/cvdd/lib/python3.10/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/envs/cvdd/lib/python3.10/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/cvdd/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/cvdd/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/cvdd/lib/python3.10/site-packages (from requests->wikipedia-api) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/cvdd/lib/python3.10/site-packages (from requests->wikipedia-api) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/cvdd/lib/python3.10/site-packages (from requests->wikipedia-api) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/cvdd/lib/python3.10/site-packages (from requests->wikipedia-api) (2025.8.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install wikipedia-api pandas numpy tqdm scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08996ae1",
   "metadata": {},
   "source": [
    "#### 1. Setup/Library Imports and Configuration Parameters\n",
    "- Installs and imports all required libraries for scrapin (wikipedia-api), data manipulation (pandas, numpy), and progress tracking (tqdm).\n",
    "\n",
    "- Sets up filesystem paths for saving outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37b4f9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config OK.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import random\n",
    "from pathlib import Path\n",
    "import wikipediaapi\n",
    "import re\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "DATA_DIR = Path(\"./data_wiki_mix\")\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Categories to scrape \n",
    "CATEGORIES = [\"Astronomy\", \"Cooking\", \"Politics\", \"Music\", \"History\"]\n",
    "\n",
    "# Caps to keep size manageable\n",
    "MAX_PER_CATEGORY = 500     # limit number of pages per category\n",
    "MAX_DEPTH = 1              # category recursion depth (0 or 1 is enough)\n",
    "\n",
    "# Filtering\n",
    "MIN_CHARS = 300            # drop very short/empty texts\n",
    "USE_SUMMARY_FIRST = True   # summaries are shorter/cleaner\n",
    "\n",
    "# Randomness\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"Config OK.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2ce7fa",
   "metadata": {},
   "source": [
    "#### 2.Wikipedia Client, Cleaning, and Fetching Helpers\n",
    "- This part of the code connects to Wikipedia, cleans the text, and collects articles from selected categories.\n",
    "- It first imports the needed libraries -wikipediaapi to read Wikipedia pages and re for text cleaning. Then it sets up a Wikipedia client (wiki) with your email as a user-agent so the program can safely access Wikipedia data.\n",
    "\n",
    "- The clean_text() function removes unwanted parts like brackets, links, and extra spaces from the text and converts everything to lowercase so it‚Äôs clean and uniform.\n",
    "\n",
    "- The fetch_category_articles() function goes to a given Wikipedia category (like ‚ÄúAstronomy‚Äù) and collects article titles and text. It fetches either the summary or the full text, cleans it, skips short or repeated pages, and also checks subcategories up to a set depth.\n",
    "\n",
    "- In the end, it returns a list of clean articles ready to be used for building the dataset, and the message Helpers ready confirms that everything is set up correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f77678b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpers ready \n"
     ]
    }
   ],
   "source": [
    "import wikipediaapi\n",
    "import re\n",
    "\n",
    "#  Define Wikipedia client with a user-agent\n",
    "wiki = wikipediaapi.Wikipedia(\n",
    "    language=\"en\",\n",
    "    user_agent=\"CVDDResearchDataset/1.0 (pavaniaddagalla.2704@gmail.com)\"  \n",
    ")\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Cleans text by removing URLs, brackets, and extra spaces.\"\"\"\n",
    "    text = re.sub(r\"\\([^)]*\\)\", \" \", text)\n",
    "    text = text.replace(\"[\", \" \").replace(\"]\", \" \")\n",
    "    text = re.sub(r\"http\\S+|www\\.\\S+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text.lower()\n",
    "\n",
    "def fetch_category_articles(category_name: str, max_articles: int, max_depth: int = 1):\n",
    "    \"\"\"Recursively fetches Wikipedia pages from a given category.\"\"\"\n",
    "    cat_page = wiki.page(f\"Category:{category_name}\")\n",
    "    if not cat_page.exists():\n",
    "        print(f\"[WARN] Category not found: {category_name}\")\n",
    "        return []\n",
    "\n",
    "    pages = []\n",
    "    seen_titles = set()\n",
    "\n",
    "    def _crawl(cat, depth):\n",
    "        nonlocal pages, seen_titles\n",
    "        if depth > max_depth or len(pages) >= max_articles:\n",
    "            return\n",
    "        for member in cat.categorymembers.values():\n",
    "            if len(pages) >= max_articles:\n",
    "                break\n",
    "            if member.ns == wikipediaapi.Namespace.MAIN:\n",
    "                if member.title not in seen_titles:\n",
    "                    text = member.summary if USE_SUMMARY_FIRST else member.text\n",
    "                    if (not text or len(text) < MIN_CHARS) and USE_SUMMARY_FIRST:\n",
    "                        text = member.text\n",
    "                    text = clean_text(text or \"\")\n",
    "                    if len(text) >= MIN_CHARS:\n",
    "                        pages.append((member.title, text))\n",
    "                        seen_titles.add(member.title)\n",
    "            elif member.ns == wikipediaapi.Namespace.CATEGORY:\n",
    "                _crawl(member, depth + 1)\n",
    "\n",
    "    _crawl(cat_page, depth=0)\n",
    "    return pages\n",
    "\n",
    "print(\"Helpers ready \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf009fb",
   "metadata": {},
   "source": [
    "#### 3.Fetching Articles and Creating the Dataset\n",
    "- This part of the code collects the cleaned Wikipedia articles from each chosen category and combines them into a single dataset.\n",
    "- It starts with an empty list called rows, then loops through each category name in CATEGORIES (like Astronomy, Cooking, etc.). \n",
    "- For each category, it prints a message showing which one is being processed and uses the fetch_category_articles() function to get the articles.\n",
    "- For every article found, it stores the title, cleaned text, and its category in the rows list. \n",
    "- After all categories are processed, the list is converted into a pandas DataFrame called df, which becomes the main dataset. \n",
    "- The line drop_duplicates(subset=[\"text\"]) removes any repeated articles, and reset_index(drop=True) resets the row numbering.\n",
    "- Finally, it prints how many total articles were collected and shows the first few rows of the dataset to confirm that the data looks correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bf28406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching category: Astronomy\n",
      "Fetched 500 pages from Astronomy\n",
      "\n",
      "Fetching category: Cooking\n",
      "Fetched 500 pages from Cooking\n",
      "\n",
      "Fetching category: Politics\n",
      "Fetched 500 pages from Politics\n",
      "\n",
      "Fetching category: Music\n",
      "Fetched 500 pages from Music\n",
      "\n",
      "Fetching category: History\n",
      "Fetched 500 pages from History\n",
      "\n",
      " Total articles: 2471\n",
      "                   title                                               text  \\\n",
      "0  Glossary of astronomy  this glossary of astronomy is a list of defini...   \n",
      "1   Outline of astronomy  the following outline is provided as an overvi...   \n",
      "2   96P sungrazer family  the 96p sungrazer family is a small group of s...   \n",
      "\n",
      "    category  \n",
      "0  Astronomy  \n",
      "1  Astronomy  \n",
      "2  Astronomy  \n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "for cat in CATEGORIES:\n",
    "    print(f\"\\nFetching category: {cat}\")\n",
    "    pages = fetch_category_articles(cat, max_articles=MAX_PER_CATEGORY, max_depth=MAX_DEPTH)\n",
    "    print(f\"Fetched {len(pages)} pages from {cat}\")\n",
    "    for title, text in pages:\n",
    "        rows.append({\"title\": title, \"text\": text, \"category\": cat})\n",
    "\n",
    "df = pd.DataFrame(rows).drop_duplicates(subset=[\"text\"]).reset_index(drop=True)\n",
    "print(\"\\n Total articles:\", len(df))\n",
    "print(df.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2489b70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Saved full dataset to: /Users/pavani/Library/CloudStorage/OneDrive-MacquarieUniversity/Session2_2025/Applications_of_Datascience/GithubClone/CVDD-PyTorch/data/data_wiki_mix/wikipedia_topic_mix.csv\n"
     ]
    }
   ],
   "source": [
    "FULL_CSV = DATA_DIR / \"wikipedia_topic_mix.csv\"\n",
    "df.to_csv(FULL_CSV, index=False)\n",
    "print(f\"\\nüíæ Saved full dataset to: {FULL_CSV.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2605554",
   "metadata": {},
   "source": [
    "#### 4.Splitting the Dataset into Train and Test Sets\n",
    "- This part divides the collected dataset into training and testing sets while keeping a balanced number of samples for each category.\n",
    "- It first creates two empty lists ‚Äî train_rows and test_rows ‚Äî to store the data for both splits. \n",
    "- For each category in CATEGORIES, it takes all articles belonging to that category, randomly shuffles them using a fixed seed for reproducibility, and calculates 80% of the data for training. - The first 80% of rows go into the training list, and the remaining 20% go into the test list.\n",
    "\n",
    "- After processing all categories, the training and test parts are combined into two separate DataFrames (train_df and test_df), which are again shuffled to mix articles from all topics. \n",
    "- These are saved as two CSV files ‚Äî wikipedia_topic_mix_train.csv and wikipedia_topic_mix_test.csv ‚Äî inside the data folder.\n",
    "\n",
    "- Finally, the code prints how many samples are in each split and confirms that both files have been successfully saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97589fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train set: 1974 | Test set: 497\n",
      " Saved to:\n",
      "  /Users/pavani/Library/CloudStorage/OneDrive-MacquarieUniversity/Session2_2025/Applications_of_Datascience/GithubClone/CVDD-PyTorch/data/data_wiki_mix/wikipedia_topic_mix_train.csv\n",
      "  /Users/pavani/Library/CloudStorage/OneDrive-MacquarieUniversity/Session2_2025/Applications_of_Datascience/GithubClone/CVDD-PyTorch/data/data_wiki_mix/wikipedia_topic_mix_test.csv\n"
     ]
    }
   ],
   "source": [
    "# ---------- Split train/test ----------\n",
    "# Random split per category to keep balance\n",
    "train_rows, test_rows = [], []\n",
    "\n",
    "for cat in CATEGORIES:\n",
    "    cat_df = df[df[\"category\"] == cat].sample(frac=1, random_state=RANDOM_SEED)\n",
    "    n_train = int(0.8 * len(cat_df))\n",
    "    train_rows.append(cat_df.iloc[:n_train])\n",
    "    test_rows.append(cat_df.iloc[n_train:])\n",
    "\n",
    "train_df = pd.concat(train_rows).sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "test_df  = pd.concat(test_rows).sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "\n",
    "TRAIN_CSV = DATA_DIR / \"wikipedia_topic_mix_train.csv\"\n",
    "TEST_CSV  = DATA_DIR / \"wikipedia_topic_mix_test.csv\"\n",
    "\n",
    "train_df.to_csv(TRAIN_CSV, index=False)\n",
    "test_df.to_csv(TEST_CSV, index=False)\n",
    "\n",
    "print(f\"\\n Train set: {len(train_df)} | Test set: {len(test_df)}\")\n",
    "print(f\" Saved to:\\n  {TRAIN_CSV.resolve()}\\n  {TEST_CSV.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccd6614",
   "metadata": {},
   "source": [
    "#### 5.Creating Synthetic Mixed Samples (Optional Step)\n",
    "- This part of the code adds extra data by creating synthetic or mixed-category text samples, which can be useful for testing how well the model detects anomalies or mixed contexts.\n",
    "\n",
    "- The function create_synthetic_mixes() takes a list of DataFrames (df_list), where each one represents a different category. It then creates new samples by combining short text pieces from multiple categories. For each sample, it picks one article from each category, takes up to max_chars_each characters from each, and joins them into a single mixed paragraph. Each mixed sample is labeled with the category \"synthetic_mix\" and stored in a new DataFrame.\n",
    "\n",
    "- Next, the code prepares one DataFrame per category and calls the function to create 150 synthetic samples. These are then combined with the original dataset using pd.concat() to form an augmented dataset that includes both real and mixed samples. The combined dataset is shuffled and saved as wikipedia_topic_mix_augmented.csv.\n",
    "\n",
    "- Finally, the script prints how many synthetic samples were generated, confirms that the file has been saved, and displays ‚ÄúDone.‚Äù to indicate that the data augmentation process is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4daeec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Synthetic samples created: 150\n",
      " Saved augmented dataset to: /Users/pavani/Library/CloudStorage/OneDrive-MacquarieUniversity/Session2_2025/Applications_of_Datascience/GithubClone/CVDD-PyTorch/data/data_wiki_mix/wikipedia_topic_mix_augmented.csv\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#---------- Optional: Create Synthetic Mixes ----------\n",
    "def create_synthetic_mixes(df_list, n_samples=100, max_chars_each=500):\n",
    "    \"\"\"Creates synthetic mixed-text samples from different categories.\"\"\"\n",
    "    if len(df_list) < 2:\n",
    "        return pd.DataFrame()\n",
    "    syn = []\n",
    "    for i in range(min(n_samples, min(len(d) for d in df_list))):\n",
    "        texts = [d.iloc[i][\"text\"][:max_chars_each] for d in df_list]\n",
    "        mixed = \" \".join(texts)\n",
    "        syn.append({\"title\": f\"SYN_{i}\", \"text\": mixed, \"category\": \"synthetic_mix\"})\n",
    "    return pd.DataFrame(syn)\n",
    "\n",
    "\n",
    "dfs = [df[df[\"category\"] == cat].reset_index(drop=True) for cat in CATEGORIES]\n",
    "synthetic_df = create_synthetic_mixes(dfs, n_samples=150)\n",
    "\n",
    "AUG_CSV = DATA_DIR / \"wikipedia_topic_mix_augmented.csv\"\n",
    "aug_df = pd.concat([df, synthetic_df], ignore_index=True).sample(frac=1, random_state=RANDOM_SEED)\n",
    "aug_df.to_csv(AUG_CSV, index=False)\n",
    "\n",
    "print(f\"\\n Synthetic samples created: {len(synthetic_df)}\")\n",
    "print(f\" Saved augmented dataset to: {AUG_CSV.resolve()}\")\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0d4046",
   "metadata": {},
   "source": [
    "#### Exporting Text Files into Category Folders\n",
    "\n",
    "- This part of the code organizes and saves the dataset into a structured folder format so that each article is stored as a separate .txt file inside its corresponding category folder.\n",
    "\n",
    "- The export_to_folders() function takes three inputs ‚Äî the base directory (base_dir), the DataFrame to export (df), and the name of the split (split_name, such as \"train\" or \"test\"). It first creates a main folder for the split (for example, data_wiki_mix/train/) and then makes a subfolder for each category (like Astronomy, Cooking, etc.).\n",
    "\n",
    "- For every category, it loops through all the articles, creates safe filenames by replacing special characters with underscores, and saves each article‚Äôs cleaned text as a separate .txt file. This ensures that each text document can be easily accessed later for training or evaluation.\n",
    "\n",
    "- After defining the function, it is called twice ‚Äî once for the training set and once for the test set ‚Äî to export all data into their respective folders. The message ‚ÄúText export complete.‚Äù confirms that all text files have been successfully created and saved in the correct structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ff3abf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 395 texts to data_wiki_mix/train/History ...\n",
      "Saving 398 texts to data_wiki_mix/train/Cooking ...\n",
      "Saving 388 texts to data_wiki_mix/train/Astronomy ...\n",
      "Saving 397 texts to data_wiki_mix/train/Politics ...\n",
      "Saving 396 texts to data_wiki_mix/train/Music ...\n",
      "Saving 99 texts to data_wiki_mix/test/History ...\n",
      "Saving 98 texts to data_wiki_mix/test/Astronomy ...\n",
      "Saving 100 texts to data_wiki_mix/test/Politics ...\n",
      "Saving 100 texts to data_wiki_mix/test/Cooking ...\n",
      "Saving 100 texts to data_wiki_mix/test/Music ...\n",
      "\n",
      "Text export complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# ---------- Create folder structure and save text files ----------\n",
    "def export_to_folders(base_dir, df, split_name):\n",
    "    \"\"\"\n",
    "    Exports texts into subfolders per category.\n",
    "    Each article becomes its own .txt file.\n",
    "    \"\"\"\n",
    "    split_dir = Path(base_dir) / split_name\n",
    "    split_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for cat in df[\"category\"].unique():\n",
    "        cat_dir = split_dir / cat\n",
    "        cat_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        cat_df = df[df[\"category\"] == cat].reset_index(drop=True)\n",
    "        print(f\"Saving {len(cat_df)} texts to {cat_dir} ...\")\n",
    "        \n",
    "        for i, row in cat_df.iterrows():\n",
    "            # Create safe file name (remove slashes or illegal chars)\n",
    "            safe_title = re.sub(r\"[^A-Za-z0-9_]+\", \"_\", row[\"title\"])[:100]\n",
    "            file_path = cat_dir / f\"{safe_title or f'article_{i}'}.txt\"\n",
    "            \n",
    "            with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(row[\"text\"])\n",
    "\n",
    "# Export both train and test splits\n",
    "export_to_folders(DATA_DIR, train_df, \"train\")\n",
    "export_to_folders(DATA_DIR, test_df, \"test\")\n",
    "\n",
    "print(\"\\nText export complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0095c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            title  \\\n",
      "0      Venus tablet of Ammisaduqa   \n",
      "1          Counterfactual history   \n",
      "2                  Pastry blender   \n",
      "3  List of astronomical societies   \n",
      "4          History of agriculture   \n",
      "\n",
      "                                                text   category  \n",
      "0  the venus tablet of ammisaduqa is the record o...    History  \n",
      "1  counterfactual history is a form of historiogr...    History  \n",
      "2  a pastry blender, or pastry cutter, is a devic...    Cooking  \n",
      "3  a list of notable groups devoted to promoting ...  Astronomy  \n",
      "4  agriculture began independently in different p...    History  \n",
      "\n",
      "Categories: ['History' 'Cooking' 'Astronomy' 'Politics' 'Music']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv(\"./data_wiki_mix/wikipedia_topic_mix_train.csv\")\n",
    "print(train.head())\n",
    "print(\"\\nCategories:\", train['category'].unique())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvdd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
